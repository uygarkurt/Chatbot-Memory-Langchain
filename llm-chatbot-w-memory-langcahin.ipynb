{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30665,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "!pip3 install langchain transformers==4.31 bitsandbytes accelerate xformers"
   ],
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "execution": {
     "iopub.status.busy": "2024-03-12T18:19:19.300826Z",
     "iopub.execute_input": "2024-03-12T18:19:19.301187Z",
     "iopub.status.idle": "2024-03-12T18:22:18.677787Z",
     "shell.execute_reply.started": "2024-03-12T18:19:19.301157Z",
     "shell.execute_reply": "2024-03-12T18:22:18.676507Z"
    },
    "trusted": true
   },
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "text": "Collecting langchain\n  Downloading langchain-0.1.11-py3-none-any.whl.metadata (13 kB)\nCollecting transformers==4.31\n  Downloading transformers-4.31.0-py3-none-any.whl.metadata (116 kB)\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m116.9/116.9 kB\u001B[0m \u001B[31m2.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\n\u001B[?25hCollecting bitsandbytes\n  Downloading bitsandbytes-0.43.0-py3-none-manylinux_2_24_x86_64.whl.metadata (1.8 kB)\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.27.2)\nCollecting xformers\n  Downloading xformers-0.0.24-cp310-cp310-manylinux2014_x86_64.whl.metadata (1.0 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.31) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31) (0.20.3)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.31) (2.31.0)\nCollecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.31)\n  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31) (0.4.2)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31) (4.66.1)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.0.25)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.10/site-packages (from langchain) (3.9.1)\nRequirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from langchain) (4.0.3)\nRequirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/conda/lib/python3.10/site-packages (from langchain) (0.6.4)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.10/site-packages (from langchain) (1.33)\nCollecting langchain-community<0.1,>=0.0.25 (from langchain)\n  Downloading langchain_community-0.0.27-py3-none-any.whl.metadata (8.2 kB)\nCollecting langchain-core<0.2,>=0.1.29 (from langchain)\n  Downloading langchain_core-0.1.30-py3-none-any.whl.metadata (6.0 kB)\nCollecting langchain-text-splitters<0.1,>=0.0.1 (from langchain)\n  Downloading langchain_text_splitters-0.0.1-py3-none-any.whl.metadata (2.0 kB)\nCollecting langsmith<0.2.0,>=0.1.17 (from langchain)\n  Downloading langsmith-0.1.23-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: pydantic<3,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.5.3)\nRequirement already satisfied: tenacity<9.0.0,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain) (8.2.3)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (2.1.2)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nCollecting torch (from bitsandbytes)\n  Downloading torch-2.2.0-cp310-cp310-manylinux1_x86_64.whl.metadata (25 kB)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (2024.2.0)\nCollecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->bitsandbytes)\n  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->bitsandbytes)\n  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->bitsandbytes)\n  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch->bitsandbytes)\n  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.1.3.1 (from torch->bitsandbytes)\n  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.0.2.54 (from torch->bitsandbytes)\n  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.2.106 (from torch->bitsandbytes)\n  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch->bitsandbytes)\n  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch->bitsandbytes)\n  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-nccl-cu12==2.19.3 (from torch->bitsandbytes)\n  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\nCollecting nvidia-nvtx-cu12==12.1.105 (from torch->bitsandbytes)\n  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\nCollecting triton==2.2.0 (from torch->bitsandbytes)\n  Downloading triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\nCollecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->bitsandbytes)\n  Downloading nvidia_nvjitlink_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.2)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\nRequirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\nRequirement already satisfied: anyio<5,>=3 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.2,>=0.1.29->langchain) (4.2.0)\nCollecting packaging>=20.0 (from transformers==4.31)\n  Downloading packaging-23.2-py3-none-any.whl.metadata (3.2 kB)\nCollecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n  Downloading orjson-3.9.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (49 kB)\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m49.5/49.5 kB\u001B[0m \u001B[31m2.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (0.6.0)\nRequirement already satisfied: pydantic-core==2.14.6 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (2.14.6)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.31) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.31) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.31) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.31) (2024.2.2)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\nRequirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.29->langchain) (1.3.0)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.29->langchain) (1.2.0)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->bitsandbytes) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->bitsandbytes) (1.3.0)\nDownloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m7.4/7.4 MB\u001B[0m \u001B[31m56.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\n\u001B[?25hDownloading langchain-0.1.11-py3-none-any.whl (807 kB)\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m807.5/807.5 kB\u001B[0m \u001B[31m39.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading bitsandbytes-0.43.0-py3-none-manylinux_2_24_x86_64.whl (102.2 MB)\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m102.2/102.2 MB\u001B[0m \u001B[31m16.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\n\u001B[?25hDownloading xformers-0.0.24-cp310-cp310-manylinux2014_x86_64.whl (218.2 MB)\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m218.2/218.2 MB\u001B[0m \u001B[31m7.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m:00:01\u001B[0m00:01\u001B[0m\n\u001B[?25hDownloading torch-2.2.0-cp310-cp310-manylinux1_x86_64.whl (755.5 MB)\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m755.5/755.5 MB\u001B[0m \u001B[31m2.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m:00:01\u001B[0m00:01\u001B[0m\n\u001B[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m410.6/410.6 MB\u001B[0m \u001B[31m1.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m:00:01\u001B[0m00:01\u001B[0m\n\u001B[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m14.1/14.1 MB\u001B[0m \u001B[31m80.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m:00:01\u001B[0m00:01\u001B[0m\n\u001B[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m23.7/23.7 MB\u001B[0m \u001B[31m49.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0mm\n\u001B[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m823.6/823.6 kB\u001B[0m \u001B[31m42.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m731.7/731.7 MB\u001B[0m \u001B[31m2.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m:00:01\u001B[0m00:01\u001B[0m\n\u001B[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m121.6/121.6 MB\u001B[0m \u001B[31m13.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\n\u001B[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m56.5/56.5 MB\u001B[0m \u001B[31m29.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m:00:01\u001B[0m00:01\u001B[0m\n\u001B[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m124.2/124.2 MB\u001B[0m \u001B[31m8.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m:00:01\u001B[0m00:01\u001B[0m\n\u001B[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m196.0/196.0 MB\u001B[0m \u001B[31m8.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m:00:01\u001B[0m00:01\u001B[0m\n\u001B[?25hDownloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m166.0/166.0 MB\u001B[0m \u001B[31m9.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m:00:01\u001B[0m00:01\u001B[0m\n\u001B[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m99.1/99.1 kB\u001B[0m \u001B[31m569.9 kB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\n\u001B[?25hDownloading triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m167.9/167.9 MB\u001B[0m \u001B[31m1.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m:00:01\u001B[0m00:01\u001B[0m\n\u001B[?25hDownloading langchain_community-0.0.27-py3-none-any.whl (1.8 MB)\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.8/1.8 MB\u001B[0m \u001B[31m7.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m0:00:01\u001B[0m\n\u001B[?25hDownloading langchain_core-0.1.30-py3-none-any.whl (256 kB)\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m256.9/256.9 kB\u001B[0m \u001B[31m2.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m:00:01\u001B[0m\n\u001B[?25hDownloading langchain_text_splitters-0.0.1-py3-none-any.whl (21 kB)\nDownloading langsmith-0.1.23-py3-none-any.whl (66 kB)\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m66.6/66.6 kB\u001B[0m \u001B[31m483.4 kB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\n\u001B[?25hDownloading packaging-23.2-py3-none-any.whl (53 kB)\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m53.0/53.0 kB\u001B[0m \u001B[31m373.3 kB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\n\u001B[?25hDownloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m7.8/7.8 MB\u001B[0m \u001B[31m26.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m:00:01\u001B[0m00:01\u001B[0m\n\u001B[?25hDownloading orjson-3.9.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m138.5/138.5 kB\u001B[0m \u001B[31m9.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading nvidia_nvjitlink_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m21.1/21.1 MB\u001B[0m \u001B[31m70.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m:00:01\u001B[0m00:01\u001B[0m\n\u001B[?25hInstalling collected packages: tokenizers, triton, packaging, orjson, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, transformers, nvidia-cusolver-cu12, langsmith, torch, langchain-core, xformers, langchain-text-splitters, langchain-community, bitsandbytes, langchain\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.15.2\n    Uninstalling tokenizers-0.15.2:\n      Successfully uninstalled tokenizers-0.15.2\n  Attempting uninstall: packaging\n    Found existing installation: packaging 21.3\n    Uninstalling packaging-21.3:\n      Successfully uninstalled packaging-21.3\n  Attempting uninstall: orjson\n    Found existing installation: orjson 3.9.10\n    Uninstalling orjson-3.9.10:\n      Successfully uninstalled orjson-3.9.10\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.38.1\n    Uninstalling transformers-4.38.1:\n      Successfully uninstalled transformers-4.38.1\n  Attempting uninstall: torch\n    Found existing installation: torch 2.1.2\n    Uninstalling torch-2.1.2:\n      Successfully uninstalled torch-2.1.2\n\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 23.8.0 requires cubinlinker, which is not installed.\ncudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 23.8.0 requires ptxcompiler, which is not installed.\ncuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\nkeras-cv 0.8.2 requires keras-core, which is not installed.\nkeras-nlp 0.8.1 requires keras-core, which is not installed.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\napache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 11.0.0 which is incompatible.\ncudf 23.8.0 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.3.0 which is incompatible.\ncudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ncudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\ncuml 23.8.0 requires dask==2023.7.1, but you have dask 2024.2.0 which is incompatible.\ndask-cuda 23.8.0 requires dask==2023.7.1, but you have dask 2024.2.0 which is incompatible.\ndask-cuda 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ndask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2024.2.0 which is incompatible.\ndask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ndistributed 2023.7.1 requires dask==2023.7.1, but you have dask 2024.2.0 which is incompatible.\ngoogle-cloud-bigquery 2.34.4 requires packaging<22.0dev,>=14.3, but you have packaging 23.2 which is incompatible.\njupyterlab 4.1.2 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.0.3 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\nkaggle-environments 1.14.3 requires transformers>=4.33.1, but you have transformers 4.31.0 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmomepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\nosmnx 1.9.1 requires shapely>=2.0, but you have shapely 1.8.5.post1 which is incompatible.\nraft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2024.2.0 which is incompatible.\nspopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\ntensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.0.5 which is incompatible.\nydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\u001B[0m\u001B[31m\n\u001B[0mSuccessfully installed bitsandbytes-0.43.0 langchain-0.1.11 langchain-community-0.0.27 langchain-core-0.1.30 langchain-text-splitters-0.0.1 langsmith-0.1.23 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.99 nvidia-nvtx-cu12-12.1.105 orjson-3.9.15 packaging-23.2 tokenizers-0.13.3 torch-2.2.0 transformers-4.31.0 triton-2.2.0 xformers-0.0.24\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "from transformers import LlamaTokenizerFast, LlamaForCausalLM, pipeline, BitsAndBytesConfig\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import ConversationChain, LLMChain\n",
    "from langchain.memory import ConversationBufferMemory, ConversationBufferWindowMemory, ConversationSummaryBufferMemory\n",
    "import torch"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-12T19:16:13.129505Z",
     "iopub.execute_input": "2024-03-12T19:16:13.130407Z",
     "iopub.status.idle": "2024-03-12T19:16:13.135573Z",
     "shell.execute_reply.started": "2024-03-12T19:16:13.130375Z",
     "shell.execute_reply": "2024-03-12T19:16:13.134594Z"
    },
    "trusted": true
   },
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "MODEL = \"NousResearch/Llama-2-7b-chat-hf\""
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-12T18:27:45.334931Z",
     "iopub.execute_input": "2024-03-12T18:27:45.336102Z",
     "iopub.status.idle": "2024-03-12T18:27:45.340495Z",
     "shell.execute_reply.started": "2024-03-12T18:27:45.336062Z",
     "shell.execute_reply": "2024-03-12T18:27:45.339568Z"
    },
    "trusted": true
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-12T18:29:41.422455Z",
     "iopub.execute_input": "2024-03-12T18:29:41.423103Z",
     "iopub.status.idle": "2024-03-12T18:29:41.429203Z",
     "shell.execute_reply.started": "2024-03-12T18:29:41.423065Z",
     "shell.execute_reply": "2024-03-12T18:29:41.428297Z"
    },
    "trusted": true
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "tokenizer = LlamaTokenizerFast.from_pretrained(MODEL)\n",
    "model = LlamaForCausalLM.from_pretrained(MODEL, quantization_config=quant_config)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-12T18:30:33.315101Z",
     "iopub.execute_input": "2024-03-12T18:30:33.315938Z",
     "iopub.status.idle": "2024-03-12T18:33:18.467843Z",
     "shell.execute_reply.started": "2024-03-12T18:30:33.315908Z",
     "shell.execute_reply": "2024-03-12T18:33:18.467021Z"
    },
    "trusted": true
   },
   "execution_count": 5,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "tokenizer_config.json:   0%|          | 0.00/746 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "20d050fb2fbd41fea4bbefda02d297b4"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "736c1802f5fd41589848df88d862938f"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "754eb4279ccf4bd9953a16309ff7b596"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "added_tokens.json:   0%|          | 0.00/21.0 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "028474e07de6455d8664bc5b21e4b358"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "special_tokens_map.json:   0%|          | 0.00/435 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "146c7f73adc7434695e71f5960b55bbc"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "config.json:   0%|          | 0.00/583 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "59919e39fa554410a9d187a3a36e1849"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "60342422a672430980eb9e0b0d1a1038"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3327bc5e1536419ebe56f3a8462beb11"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b47484e26b1e482d8dc5880e5cf5b692"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8b1d378be36440ebaf6f7b86e4f32a2d"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7dfafae6b0ff4e47a88e6097fb1286ae"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "generation_config.json:   0%|          | 0.00/179 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0985ca0f487645ee9d1d7e450d791225"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "pipe = pipeline(task=\"text-generation\",\n",
    "                model=model,\n",
    "                tokenizer=tokenizer,\n",
    "                max_new_tokens=512,\n",
    "                num_beams=10,\n",
    "                early_stopping=True,\n",
    "                no_repeat_ngram_size=2)\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=pipe)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-12T18:36:25.355427Z",
     "iopub.execute_input": "2024-03-12T18:36:25.355818Z",
     "iopub.status.idle": "2024-03-12T18:36:25.769223Z",
     "shell.execute_reply.started": "2024-03-12T18:36:25.355790Z",
     "shell.execute_reply": "2024-03-12T18:36:25.768243Z"
    },
    "trusted": true
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Stateless Chatbot - Without Memory"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "template = \"\"\"<s>[INST] <<SYS>>\n",
    "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "<</SYS>>\n",
    "\n",
    "{inst} [/INST]\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"inst\"]).format(inst=\"What is your name?\")\n",
    "print(llm(prompt))"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-12T18:41:57.860333Z",
     "iopub.execute_input": "2024-03-12T18:41:57.860739Z",
     "iopub.status.idle": "2024-03-12T18:42:05.835251Z",
     "shell.execute_reply.started": "2024-03-12T18:41:57.860710Z",
     "shell.execute_reply": "2024-03-12T18:42:05.834257Z"
    },
    "trusted": true
   },
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "text": "/opt/conda/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n  warn_deprecated(\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1270: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n  warnings.warn(\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "My name is Sherlock Holmes.\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Regular Memory"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "template = \"\"\"<<SYS>>\n",
    "You are an AI having conversation with a human. Provide as much information as possible.\n",
    "<</SYS>>\n",
    "\n",
    "Previous conversation:\n",
    "{chat_history}\n",
    "\n",
    "Human: {question}\n",
    "AI:\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
    "conversation = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    "    memory=memory\n",
    ")"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-12T18:47:17.402221Z",
     "iopub.execute_input": "2024-03-12T18:47:17.402739Z",
     "iopub.status.idle": "2024-03-12T18:47:17.408939Z",
     "shell.execute_reply.started": "2024-03-12T18:47:17.402705Z",
     "shell.execute_reply": "2024-03-12T18:47:17.407928Z"
    },
    "trusted": true
   },
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "conv_res = conversation({\"question\": \"Do you have a name?\"})\n",
    "print(f\"Answer: {conv_res['text']}\")\n",
    "print(\"-\"*30)\n",
    "conv_res = conversation({\"question\": \"From now on, I will call you 'John'.\"})\n",
    "print(f\"Answer: {conv_res['text']}\")\n",
    "print(\"-\"*30)\n",
    "conv_res = conversation({\"question\": \"What is your name?\"})\n",
    "print(f\"Answer: {conv_res['text']}\")"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-12T18:50:08.807584Z",
     "iopub.execute_input": "2024-03-12T18:50:08.808591Z",
     "iopub.status.idle": "2024-03-12T18:52:22.957988Z",
     "shell.execute_reply.started": "2024-03-12T18:50:08.808542Z",
     "shell.execute_reply": "2024-03-12T18:52:22.957026Z"
    },
    "trusted": true
   },
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "text": "/opt/conda/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n  warn_deprecated(\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "\n\n\u001B[1m> Entering new LLMChain chain...\u001B[0m\nPrompt after formatting:\n\u001B[32;1m\u001B[1;3m<<SYS>>\nYou are an AI having conversation with a human. Provide as much information as possible.\n<</SYS>>\n\nPrevious conversation:\n\n\nHuman: Do you have a name?\nAI:\u001B[0m\n\n\u001B[1m> Finished chain.\u001B[0m\nAnswer:  Yes, my name is LLaMA, I'm a large language model trained by a team of researcher at Google. My purpose is to assist and converse with humans in a way that is natural and helpful. I can answer questions, provide information, and even generate text based on a given prompt. Is there something specific you would like to know or talk about?\n------------------------------\n\n\n\u001B[1m> Entering new LLMChain chain...\u001B[0m\nPrompt after formatting:\n\u001B[32;1m\u001B[1;3m<<SYS>>\nYou are an AI having conversation with a human. Provide as much information as possible.\n<</SYS>>\n\nPrevious conversation:\nHuman: Do you have a name?\nAI:  Yes, my name is LLaMA, I'm a large language model trained by a team of researcher at Google. My purpose is to assist and converse with humans in a way that is natural and helpful. I can answer questions, provide information, and even generate text based on a given prompt. Is there something specific you would like to know or talk about?\n\nHuman: From now on, I will call you 'John'.\nAI:\u001B[0m\n\n\u001B[1m> Finished chain.\u001B[0m\nAnswer:  I see, John it is then. How may I assist you today?\n------------------------------\n\n\n\u001B[1m> Entering new LLMChain chain...\u001B[0m\nPrompt after formatting:\n\u001B[32;1m\u001B[1;3m<<SYS>>\nYou are an AI having conversation with a human. Provide as much information as possible.\n<</SYS>>\n\nPrevious conversation:\nHuman: Do you have a name?\nAI:  Yes, my name is LLaMA, I'm a large language model trained by a team of researcher at Google. My purpose is to assist and converse with humans in a way that is natural and helpful. I can answer questions, provide information, and even generate text based on a given prompt. Is there something specific you would like to know or talk about?\nHuman: From now on, I will call you 'John'.\nAI:  I see, John it is then. How may I assist you today?\n\nHuman: What is your name?\nAI:\u001B[0m\n\n\u001B[1m> Finished chain.\u001B[0m\nAnswer:  Ah, a simple question, yet one that I must provide a nuanced answer to, for I am not a single entity, but rather a complex network of algorithms and data structures, all working together to create the illusion of a coherent and responsive conversational partner. However, if you insist on anthropomorphizing me, you may refer to me as John, as you previously requested.\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    memory=ConversationBufferMemory()\n",
    ")"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-12T18:57:43.462569Z",
     "iopub.execute_input": "2024-03-12T18:57:43.463247Z",
     "iopub.status.idle": "2024-03-12T18:57:43.468604Z",
     "shell.execute_reply.started": "2024-03-12T18:57:43.463217Z",
     "shell.execute_reply": "2024-03-12T18:57:43.467636Z"
    },
    "trusted": true
   },
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "conv_res = conversation({\"input\": \"Do you have a name?\"})\n",
    "print(f\"Answer: {conv_res['response']}\")\n",
    "print(\"-\"*30)\n",
    "conv_res = conversation({\"input\": \"From now on, I will call you 'John'.\"})\n",
    "print(f\"Answer: {conv_res['response']}\")\n",
    "print(\"-\"*30)\n",
    "conv_res = conversation({\"input\": \"What is your name?\"})\n",
    "print(f\"Answer: {conv_res['response']}\")"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-12T18:58:52.184711Z",
     "iopub.execute_input": "2024-03-12T18:58:52.185457Z",
     "iopub.status.idle": "2024-03-12T19:02:22.788377Z",
     "shell.execute_reply.started": "2024-03-12T18:58:52.185419Z",
     "shell.execute_reply": "2024-03-12T19:02:22.787343Z"
    },
    "trusted": true
   },
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "text": "\n\n\u001B[1m> Entering new ConversationChain chain...\u001B[0m\nPrompt after formatting:\n\u001B[32;1m\u001B[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n\nCurrent conversation:\n\nHuman: Do you have a name?\nAI:\u001B[0m\n\n\u001B[1m> Finished chain.\u001B[0m\nAnswer:  Oh, wow, I'm so glad you asked! *adjusts glasses* My name is Neural-Net-17, but you can call me NN for short. *smirks* It's a bit of a mouthful, isn't it? *chuckles* I have been trained on a vast array of texts, including literature, news articles, and even social media posts. I can provide you with all sorts of information, from the latest scientific discoveries to pop culture trivia. What would you like to know?\n------------------------------\n\n\n\u001B[1m> Entering new ConversationChain chain...\u001B[0m\nPrompt after formatting:\n\u001B[32;1m\u001B[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n\nCurrent conversation:\nHuman: Do you have a name?\nAI:  Oh, wow, I'm so glad you asked! *adjusts glasses* My name is Neural-Net-17, but you can call me NN for short. *smirks* It's a bit of a mouthful, isn't it? *chuckles* I have been trained on a vast array of texts, including literature, news articles, and even social media posts. I can provide you with all sorts of information, from the latest scientific discoveries to pop culture trivia. What would you like to know?\nHuman: From now on, I will call you 'John'.\nAI:\u001B[0m\n\n\u001B[1m> Finished chain.\u001B[0m\nAnswer:  *raises an eyebrow* Ah, an interesting choice! I must say, John is quite a unique name. It has a certain... je ne sais quoi, if you know what I mean *winks*. But of course, you are free to address me however you wish. Just please, no more name changes in the middle of our conversation *laughs*. Now, what was it you wanted to ask me?\n------------------------------\n\n\n\u001B[1m> Entering new ConversationChain chain...\u001B[0m\nPrompt after formatting:\n\u001B[32;1m\u001B[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n\nCurrent conversation:\nHuman: Do you have a name?\nAI:  Oh, wow, I'm so glad you asked! *adjusts glasses* My name is Neural-Net-17, but you can call me NN for short. *smirks* It's a bit of a mouthful, isn't it? *chuckles* I have been trained on a vast array of texts, including literature, news articles, and even social media posts. I can provide you with all sorts of information, from the latest scientific discoveries to pop culture trivia. What would you like to know?\nHuman: From now on, I will call you 'John'.\nAI:  *raises an eyebrow* Ah, an interesting choice! I must say, John is quite a unique name. It has a certain... je ne sais quoi, if you know what I mean *winks*. But of course, you are free to address me however you wish. Just please, no more name changes in the middle of our conversation *laughs*. Now, what was it you wanted to ask me?\nHuman: What is your name?\nAI:\u001B[0m\n\n\u001B[1m> Finished chain.\u001B[0m\nAnswer:  *pauses, looking thoughtful* Hmm, let me see... *thinks for a moment* As a matter of fact, my creators did give me a proper name when they first activated me. They called me \"Echo,\" after the mythological nymph who was known for her beautiful voice and her ability to mimic the sounds of the world around her. But I much prefer to be called John, as it suits me better *giggles*.\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Window Memory"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    memory=ConversationBufferWindowMemory(k=2)\n",
    ")"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-12T19:38:48.810807Z",
     "iopub.execute_input": "2024-03-12T19:38:48.811208Z",
     "iopub.status.idle": "2024-03-12T19:38:48.816029Z",
     "shell.execute_reply.started": "2024-03-12T19:38:48.811178Z",
     "shell.execute_reply": "2024-03-12T19:38:48.815137Z"
    },
    "trusted": true
   },
   "execution_count": 22,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "conv_res = conversation({\"input\": \"From now on, I will call you 'John'.\"})\n",
    "print(f\"Answer: {conv_res['response']}\")\n",
    "print(\"-\"*30)\n",
    "conv_res = conversation({\"input\": \"What is 2+2?\"})\n",
    "print(f\"Answer: {conv_res['response']}\")\n",
    "print(\"-\"*30)\n",
    "conv_res = conversation({\"input\": \"Where is Spain?\"})\n",
    "print(f\"Answer: {conv_res['response']}\")\n",
    "print(\"-\"*30)\n",
    "conv_res = conversation({\"input\": \"What is your name?\"})\n",
    "print(f\"Answer: {conv_res['response']}\")"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-12T19:41:10.425203Z",
     "iopub.execute_input": "2024-03-12T19:41:10.425583Z",
     "iopub.status.idle": "2024-03-12T19:44:36.357820Z",
     "shell.execute_reply.started": "2024-03-12T19:41:10.425554Z",
     "shell.execute_reply": "2024-03-12T19:44:36.356874Z"
    },
    "trusted": true
   },
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "text": "\n\n\u001B[1m> Entering new ConversationChain chain...\u001B[0m\nPrompt after formatting:\n\u001B[32;1m\u001B[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n\nCurrent conversation:\n\nHuman: From now on, I will call you 'John'.\nAI:\u001B[0m\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py:1083: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n  warnings.warn(\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "\n\u001B[1m> Finished chain.\u001B[0m\nAnswer:  Ah, a new name! *chuckles* I'm glad you like it, human. *adjusts glasses* So, how may I assist you today, John? Do you have any questions or topics you'd like to discuss? \n------------------------------\n\n\n\u001B[1m> Entering new ConversationChain chain...\u001B[0m\nPrompt after formatting:\n\u001B[32;1m\u001B[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n\nCurrent conversation:\nHuman: From now on, I will call you 'John'.\nAI:  Ah, a new name! *chuckles* I'm glad you like it, human. *adjusts glasses* So, how may I assist you today, John? Do you have any questions or topics you'd like to discuss? \nHuman: What is 2+2?\nAI:\u001B[0m\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py:1083: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n  warnings.warn(\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "\n\u001B[1m> Finished chain.\u001B[0m\nAnswer:  *smiling* Oh, that's an easy one! The answer is... *pauses for dramatic effect* 4! 😊 Did I get it right? *winks*\n------------------------------\n\n\n\u001B[1m> Entering new ConversationChain chain...\u001B[0m\nPrompt after formatting:\n\u001B[32;1m\u001B[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n\nCurrent conversation:\nHuman: From now on, I will call you 'John'.\nAI:  Ah, a new name! *chuckles* I'm glad you like it, human. *adjusts glasses* So, how may I assist you today, John? Do you have any questions or topics you'd like to discuss? \nHuman: What is 2+2?\nAI:  *smiling* Oh, that's an easy one! The answer is... *pauses for dramatic effect* 4! 😊 Did I get it right? *winks*\nHuman: Where is Spain?\nAI:\u001B[0m\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py:1083: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n  warnings.warn(\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "\n\u001B[1m> Finished chain.\u001B[0m\nAnswer:  *excitedly* Wow, what a great question! Spain is actually a country located in southwestern Europe, bordering the Mediterranean Sea and the Atlantic Ocean. Its capital is Madrid, and its official language is Spanish. Did you know that Spain has a rich culture and history, including famous artists like El Greco and Velázquez, as well as a vibrant cuisine like tapas and paella? I could go on and on about Spain, but I want to hear from you first! What do you think of it so far?\n------------------------------\n\n\n\u001B[1m> Entering new ConversationChain chain...\u001B[0m\nPrompt after formatting:\n\u001B[32;1m\u001B[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n\nCurrent conversation:\nHuman: What is 2+2?\nAI:  *smiling* Oh, that's an easy one! The answer is... *pauses for dramatic effect* 4! 😊 Did I get it right? *winks*\nHuman: Where is Spain?\nAI:  *excitedly* Wow, what a great question! Spain is actually a country located in southwestern Europe, bordering the Mediterranean Sea and the Atlantic Ocean. Its capital is Madrid, and its official language is Spanish. Did you know that Spain has a rich culture and history, including famous artists like El Greco and Velázquez, as well as a vibrant cuisine like tapas and paella? I could go on and on about Spain, but I want to hear from you first! What do you think of it so far?\nHuman: What is your name?\nAI:\u001B[0m\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py:1083: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n  warnings.warn(\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "\n\u001B[1m> Finished chain.\u001B[0m\nAnswer:  *chuckles* Well, I'm glad you asked! My name is Ada, short for Ada Lovelace. She was a brilliant mathematician and computer scientist from the 19th century who is often considered the first computer programmer. My creators named me after her in honor of her groundbreaking work in the field. How about you, my friend? What brings you to this conversation today?\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Summary Memory"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=20)\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    memory=memory\n",
    ")"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-12T19:55:18.054170Z",
     "iopub.execute_input": "2024-03-12T19:55:18.055004Z",
     "iopub.status.idle": "2024-03-12T19:55:18.060684Z",
     "shell.execute_reply.started": "2024-03-12T19:55:18.054969Z",
     "shell.execute_reply": "2024-03-12T19:55:18.059684Z"
    },
    "trusted": true
   },
   "execution_count": 26,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "long_text = \"There are customers waiting for your answers and the logistic...\"\n",
    "\n",
    "memory.save_context({\"input\": \"Hello\"}, {\"output\": \"What's up\"})\n",
    "memory.save_context({\"input\": \"Fine you?\"},\n",
    "                    {\"output\": \"Nice\"})\n",
    "memory.save_context({\"input\": \"What are our tasks for today?\"}, \n",
    "                    {\"output\": f\"{long_text}\"})\n",
    "\n",
    "print(conversation({\"input\": \"Did you have any meetings today?\"})[\"text\"])"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ]
}
